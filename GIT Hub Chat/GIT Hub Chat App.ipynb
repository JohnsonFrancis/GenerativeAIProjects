{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9691b12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-11-21T11:06:49.696080+05:30\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.5\n",
      "IPython version      : 8.15.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109c875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either you can store the  OpenAI key in the â€œOPENAI_API_KEYâ€ environment variable.\n",
    "# or pass it here as below from a config.ini\n",
    "import configparser\n",
    "workingFolder='C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\'\n",
    "# Read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(workingFolder+'\\\\config.ini')\n",
    "OPENAI_API_KEY=config.get('General','OPENAI_API_KEY')\n",
    "ACTIVELOOP_TOKEN=config.get('General','ACTIVELOOP_TOKEN')\n",
    "ACTIVELOOP_ORG_ID=config.get('General','ACTIVELOOP_ORG_ID')\n",
    "HUGGINGFACEHUB_API_TOKEN=config.get('General','HUGGINGFACEHUB_API_TOKEN')\n",
    "GOOGLE_API_KEY=config.get('General','GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID=config.get('General','GOOGLE_CSE_ID')\n",
    "COHERE_API_KEY=config.get('General','COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b21d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token from OPENAI/Active loop website before this. Now we are taking from the config.ini\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_ORG_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922897a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain     : 0.0.336\n",
      "streamlit     : 1.28.2\n",
      "streamlit_chat: 0.1.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import streamlit\n",
    "import streamlit_chat\n",
    "%watermark --iversions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd909f",
   "metadata": {},
   "source": [
    "## Chat with a GitHub Repository\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Large language models (LLMs) accomplish a remarkable level of language comprehension during their training process. It enables them to generate human-like text and creates powerful representations from textual data. We already covered leveraging LangChain to use LLMs for writing content with hands-on projects.\n",
    "\n",
    "This will focus on using the language models for generating embeddings from corpora. The mentioned representation will power a chat application that can answer questions from any text by finding the closest data point to an inquiry. This project focuses on finding answers from a GitHub repositoryâ€™s text files like .md and .txt. So, we will start by capturing data from a GitHub repository and converting it to embeddings. These embeddings will be saved on the Activeloopâ€™s Deep Lake vector database for fast and easy access. The Deep Lakeâ€™s retriever object will find the related files based on the userâ€™s query and provide them as the context to the model. Lastly, the model leverages the provided information to the best of its ability to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f8072",
   "metadata": {},
   "source": [
    "1) Processing the Files \n",
    "\n",
    "2) Saving the Embedding \n",
    "\n",
    "3) Retrieving from Database \n",
    "\n",
    "4) Creating an Interface.\n",
    "\n",
    "### Processing the Repository Files\n",
    "\n",
    "In order to access the files in the target repository, the script will clone the desired repository onto your computer, placing the files in a folder named \"repos\". Once we download the files, it is a matter of looping through the directory to create a list of files. It is possible to filter out specific extensions or environmental items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a250e",
   "metadata": {},
   "source": [
    "Run git clone in gitbash to clone your targetted repo and note down the root directory.\n",
    "\n",
    "git clone https://github.com/peterw/Chat-with-Git-Repo.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ef9142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='black\\nflake8', metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\dev-requirements.txt'}), Document(page_content=\"# Chat-with-Github-Repo\\n\\nThis repository contains Python scripts that demonstrate how to create a chatbot using Streamlit, OpenAI GPT-3.5-turbo, and Activeloop's Deep Lake.\\n\\nThe chatbot searches a dataset stored in Deep Lake to find relevant information from any Git repository and generates responses based on the user's input.\\n\\n## Files\\n\\n- `src/utils/process.py`: This script clones a Git repository, processes the text documents, computes embeddings using OpenAIEmbeddings, and stores the embeddings in a DeepLake instance.\\n\\n- `src/utils/chat.py`: This script creates a Streamlit web application that interacts with the user and the DeepLake instance to generate chatbot responses using OpenAI GPT-3.5-turbo.\\n\\n- `src/main.py`: This script contains the command line interface (CLI) that allows you to run the chatbot application.\\n\\n## Setup\\n\\nBefore getting started, be sure to sign up for an [Activeloop](https://www.activeloop.ai/) and [OpenAI](https://openai.com/) account and create API keys.\\n\\nTo set up and run this project, follow these steps:\\n\\n1. Clone the repository and navigate to the project directory:\\n\\n```bash\\ngit clone https://github.com/peterw/Chat-with-Git-Repo.git\\ncd Chat-with-Git-Repo\\n```\\n\\n2. Install the required packages with `pip`:\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\nFor development dependencies, you can install them using the following command:\\n\\n```bash\\npip install -r dev-requirements.txt\\n```\\n\\n3. Set the environment variables:\\n\\nCopy the `.env.example` file:\\n\\n```bash\\ncp .env.example .env\\n```\\n\\nProvide your API keys and username:\\n\\n```\\nOPENAI_API_KEY=your_openai_api_key\\nACTIVELOOP_TOKEN=your_activeloop_api_token\\nACTIVELOOP_USERNAME=your_activeloop_username\\n```\\n\\n4. Use the CLI to run the chatbot application. You can either process a Git repository or start the chat application using an existing dataset.\\n\\n> For complete CLI instructions run `python src/main.py --help`\\n\\nTo process a Git repository, use the `process` subcommand:\\n\\n```bash\\npython src/main.py process --repo-url https://github.com/username/repo_name\\n```\\n\\nYou can also specify additional options, such as file extensions to include while processing the repository, the name for the Activeloop dataset, or the destination to clone the repository:\\n\\n```bash\\npython src/main.py process --repo-url https://github.com/username/repo_name --include-file-extensions .md .txt --activeloop-dataset-name my-dataset --repo-destination repos\\n```\\n\\nTo start the chat application using an existing dataset, use the `chat` subcommand:\\n\\n```bash\\npython src/main.py chat --activeloop-dataset-name my-dataset\\n```\\n\\nThe Streamlit chat app will run, and you can interact with the chatbot at `http://localhost:8501` (or the next available port) to ask questions about the repository.\\n\\n## Sponsors\\n\\nâœ¨ Learn to build projects like this one (early bird discount): [BuildFast Course](https://www.buildfast.academy)\\n\\n## License\\n\\n[MIT License](LICENSE)\", metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\README.md'}), Document(page_content='deeplake\\nlangchain\\nopenai\\npathspec\\npython-dotenv\\nstreamlit\\nstreamlit_chat\\ntiktoken', metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\requirements.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = workingFolder + \"Chat-with-Github-Repo\"\n",
    "docs = []\n",
    "file_extensions = ['.md', '.txt']\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):    \n",
    "    for file in filenames:\n",
    "        file_path = os.path.join(dirpath, file)\n",
    "        if file_extensions and os.path.splitext(file)[1] not in file_extensions:\n",
    "            continue\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        docs.extend(loader.load_and_split())\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb2619",
   "metadata": {},
   "source": [
    "The sample code above creates a list of all the files in a repository. It is possible to filter each item by extension types like file_extensions=['.md', '.txt'] which only focus on markdown and text files. \n",
    "\n",
    "Now that the list of files are created, the split_documents method from the CharacterTextSplitter class in the LangChain library will read the files and split their contents into chunks of 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977b2f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='black\\nflake8', metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\dev-requirements.txt'}), Document(page_content=\"# Chat-with-Github-Repo\\n\\nThis repository contains Python scripts that demonstrate how to create a chatbot using Streamlit, OpenAI GPT-3.5-turbo, and Activeloop's Deep Lake.\\n\\nThe chatbot searches a dataset stored in Deep Lake to find relevant information from any Git repository and generates responses based on the user's input.\\n\\n## Files\\n\\n- `src/utils/process.py`: This script clones a Git repository, processes the text documents, computes embeddings using OpenAIEmbeddings, and stores the embeddings in a DeepLake instance.\\n\\n- `src/utils/chat.py`: This script creates a Streamlit web application that interacts with the user and the DeepLake instance to generate chatbot responses using OpenAI GPT-3.5-turbo.\\n\\n- `src/main.py`: This script contains the command line interface (CLI) that allows you to run the chatbot application.\\n\\n## Setup\\n\\nBefore getting started, be sure to sign up for an [Activeloop](https://www.activeloop.ai/) and [OpenAI](https://openai.com/) account and create API keys.\", metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\README.md'}), Document(page_content='To set up and run this project, follow these steps:\\n\\n1. Clone the repository and navigate to the project directory:\\n\\n```bash\\ngit clone https://github.com/peterw/Chat-with-Git-Repo.git\\ncd Chat-with-Git-Repo\\n```\\n\\n2. Install the required packages with `pip`:\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\nFor development dependencies, you can install them using the following command:\\n\\n```bash\\npip install -r dev-requirements.txt\\n```\\n\\n3. Set the environment variables:\\n\\nCopy the `.env.example` file:\\n\\n```bash\\ncp .env.example .env\\n```\\n\\nProvide your API keys and username:\\n\\n```\\nOPENAI_API_KEY=your_openai_api_key\\nACTIVELOOP_TOKEN=your_activeloop_api_token\\nACTIVELOOP_USERNAME=your_activeloop_username\\n```\\n\\n4. Use the CLI to run the chatbot application. You can either process a Git repository or start the chat application using an existing dataset.\\n\\n> For complete CLI instructions run `python src/main.py --help`\\n\\nTo process a Git repository, use the `process` subcommand:', metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\README.md'}), Document(page_content='```bash\\npython src/main.py process --repo-url https://github.com/username/repo_name\\n```\\n\\nYou can also specify additional options, such as file extensions to include while processing the repository, the name for the Activeloop dataset, or the destination to clone the repository:\\n\\n```bash\\npython src/main.py process --repo-url https://github.com/username/repo_name --include-file-extensions .md .txt --activeloop-dataset-name my-dataset --repo-destination repos\\n```\\n\\nTo start the chat application using an existing dataset, use the `chat` subcommand:\\n\\n```bash\\npython src/main.py chat --activeloop-dataset-name my-dataset\\n```\\n\\nThe Streamlit chat app will run, and you can interact with the chatbot at `http://localhost:8501` (or the next available port) to ask questions about the repository.\\n\\n## Sponsors\\n\\nâœ¨ Learn to build projects like this one (early bird discount): [BuildFast Course](https://www.buildfast.academy)\\n\\n## License\\n\\n[MIT License](LICENSE)', metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\README.md'}), Document(page_content='deeplake\\nlangchain\\nopenai\\npathspec\\npython-dotenv\\nstreamlit\\nstreamlit_chat\\ntiktoken', metadata={'source': 'C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\Chat-with-Github-Repo\\\\requirements.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splitted_text = text_splitter.split_documents(docs)\n",
    "\n",
    "print(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0a485",
   "metadata": {},
   "source": [
    "The splitted_text variable holds the textual content which is ready to be converted to embedding representations.\n",
    "\n",
    "### Saving the Embeddings\n",
    "\n",
    "Letâ€™s create the database before going through the process of converting texts to embeddings. It is where the integration between LangChain and Deep Lake comes in handy! We initialize the database in cloud using the hub://... format and the OpenAIEmbeddings() from LangChain as the embedding function. The Deep Lake library will iterate through the content and generate the embedding automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8a077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 5 embeddings in 1 batches of size 5:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://jfrancis/langchain_course_chat_with_gh', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (5, 1)      str     None   \n",
      " metadata     json      (5, 1)      str     None   \n",
      " embedding  embedding  (5, 1536)  float32   None   \n",
      "    id        text      (5, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c4f6a1f8-8835-11ee-b60b-401c83da435e',\n",
       " 'c4f6a1f9-8835-11ee-98a7-401c83da435e',\n",
       " 'c4f6a1fa-8835-11ee-b082-401c83da435e',\n",
       " 'c4f6a1fb-8835-11ee-b8f9-401c83da435e',\n",
       " 'c4f6a1fc-8835-11ee-9aea-401c83da435e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the â€œOPENAI_API_KEYâ€ environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_ORG_ID\n",
    "my_activeloop_dataset_name = \"langchain_course_chat_with_gh\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83480a",
   "metadata": {},
   "source": [
    "### Retrieving from Database\n",
    "\n",
    "The last step is to code the process to answer the userâ€™s question based on the databaseâ€™s information. Once again, the integration of LangChain and Deep Lake simplifies the process significantly, making it exceptionally easy. We need 1) a retriever object from the Deep Lake database using the .as_retriever() method, and 2) a conversational model like ChatGPT using the ChatOpenAI() class.\n",
    "\n",
    "Finally, LangChainâ€™s RetrievalQA class ties everything together! It uses the userâ€™s input as the prompt while including the results from the database as the context. So, the ChatGPT model can find the correct one from the provided context. It is worth noting that the database retriever is configured to gather instances closely related to the userâ€™s query by utilizing cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd801e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The repository\\'s name is \"Chat-with-Github-Repo\".'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a retriever from the DeepLake instance\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Set the search parameters for the retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# Create a ChatOpenAI model instance\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a RetrievalQA instance from the model and retriever\n",
    "qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# Return the result of the query\n",
    "qa.run(\"What is the repository's name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ceac3",
   "metadata": {},
   "source": [
    "### Create an Interface\n",
    "\n",
    "Creating a user interface (UI) for the bot to be accessed through a web browser is an optional yet crucial step. This addition will elevate your ideas to new heights, allowing users to engage with the application effortlessly, even without any programming expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bce70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "# Set the title for the Streamlit app\n",
    "st.title(f\"Chat with GitHub Repository\")\n",
    "\n",
    "# Initialize the session state for placeholder messages.\n",
    "if \"generated\" not in st.session_state:\n",
    "    st.session_state[\"generated\"] = [\"i am ready to help you ser\"]\n",
    "\n",
    "if \"past\" not in st.session_state:\n",
    "    st.session_state[\"past\"] = [\"hello\"]\n",
    "\n",
    "# A field input to receive user queries\n",
    "input_text = st.text_input(\"\", key=\"input\")\n",
    "\n",
    "# Search the databse and add the responses to state\n",
    "if input_text:\n",
    "    output = qa.run(input_text)\n",
    "    st.session_state.past.append(input_text)\n",
    "    st.session_state.generated.append(output)\n",
    "\n",
    "# Create the conversational UI using the previous states\n",
    "if st.session_state[\"generated\"]:\n",
    "    for i in range(len(st.session_state[\"generated\"])):\n",
    "        message(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "        message(st.session_state[\"generated\"][i], key=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45ae6f",
   "metadata": {},
   "source": [
    "The code above is straightforward. We call st.text_input() to create text input for users queries. The query will be passed to the previously declared RetrievalQA object, and the results will be shown using the message component. You should store the mentioned code in a Python file (for example, chat.py) and run the following command to see the interface locally.\n",
    "\n",
    "Please read the documentation https://docs.streamlit.io/library/get-started on how to deploy https://docs.streamlit.io/library/get-started/create-an-app#share-your-app the application on the web so anyone can access it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9a218",
   "metadata": {},
   "source": [
    "### Putting Everything Together\n",
    "\n",
    "Below is the summing up with all code. As we already created the the vector database in active lake. This code will just read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6356bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "@st.cache_resource  # This will run only once\n",
    "def get_llm_qa():\n",
    "    workingFolder='C:\\\\Users\\\\jfrancis\\\\OneDrive - GalaxE. Solutions, Inc\\\\GalaxE D Drive\\\\AI Journey\\\\Gen AI\\\\'\n",
    "    # Read the configuration file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(workingFolder+'\\\\config.ini')\n",
    "    OPENAI_API_KEY=config.get('General','OPENAI_API_KEY')\n",
    "    ACTIVELOOP_TOKEN=config.get('General','ACTIVELOOP_TOKEN')\n",
    "    ACTIVELOOP_ORG_ID=config.get('General','ACTIVELOOP_ORG_ID')\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "    my_activeloop_org_id = ACTIVELOOP_ORG_ID\n",
    "\n",
    "    # Read from activeloop vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    my_activeloop_org_id = ACTIVELOOP_ORG_ID\n",
    "    my_activeloop_dataset_name = \"langchain_course_chat_with_gh\"\n",
    "    dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "    db = DeepLake(dataset_path=dataset_path, read_only=True, embedding=embeddings)\n",
    "\n",
    "    # Retrieval queue from activeloop\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "    retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "    retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "    retriever.search_kwargs[\"k\"] = 10\n",
    "    model = ChatOpenAI()\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "    st.success(\"Loaded RetrievalQA\")  # ðŸ‘ˆ Show a success message\n",
    "    return qa\n",
    "\n",
    "qa = get_llm_qa()\n",
    "\n",
    "# Design the front end chat app\n",
    "st.title(f\"Chat with GitHub Repository\")\n",
    "if \"generated\" not in st.session_state:\n",
    "    st.session_state[\"generated\"] = [\"i am ready to help you ser\"]\n",
    "if \"past\" not in st.session_state:\n",
    "    st.session_state[\"past\"] = [\"hello\"]\n",
    "input_text = st.text_input(\"\", key=\"input\")\n",
    "if input_text:\n",
    "    output = qa.run(input_text)\n",
    "    st.session_state.past.append(input_text)\n",
    "    st.session_state.generated.append(output)\n",
    "if st.session_state[\"generated\"]:\n",
    "    for i in range(len(st.session_state[\"generated\"])):\n",
    "        message(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "        message(st.session_state[\"generated\"][i], key=str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

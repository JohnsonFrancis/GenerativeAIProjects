{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fa18b8",
   "metadata": {},
   "source": [
    "### Bloom-560m \n",
    "##### It is a large open-science, open-access, multilingual language model developed by BigScience. It is a text generation model that has been trained on a large amount of data and is capable of generating coherent and contextually relevant text based on a given prompt.\n",
    "##### The model is an alternative to OpenAI’s popular ChatGPT 34. It can be used in various applications such as natural language processing, chatbots, and content generation\n",
    "\n",
    "###### More details on this experiment is available at https://towardsdatascience.com/getting-started-with-bloom-9e3295459b65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef411220",
   "metadata": {},
   "source": [
    "##### pip install transformers\n",
    "##### pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a9abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dd12c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596d8581281c4ae7815eb55e5304b611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854725fb161c4b918b88f11281ed7007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336a9a5d3b454d01aee145390e6ac0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855408a209074e628a5db2ad5d9b7a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f08daee3f7146af882e2dd0c81da5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")  --> 330GB dowload\n",
    "#tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534bf25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"It was a dark and stormy night\"\n",
    "result_length = 50\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8926d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, and the wind was blowing in the\n",
      "direction of the west. The wind was blowing in the direction of the\n",
      "west, and the wind was blowing in the direction of the west. The\n",
      "wind was\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=result_length\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db51d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, and the wind was blowing in the\n",
      "direction of the west. The sun was still shining, but it was not\n",
      "very bright, for the clouds were thick and heavy. It was very dark\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "# Beam Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171e559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. There was only one way I could turn the wind away, so I did.\n",
      "I tried, I tried, but it was too late. We had a few hours before the clock struck midnight when she made\n"
     ]
    }
   ],
   "source": [
    "# Sampling Top-k + Top-p\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

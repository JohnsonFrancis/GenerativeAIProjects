{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7fb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either you can store the  OpenAI key in the “OPENAI_API_KEY” environment variable.\n",
    "# or pass it here as below from a config.ini\n",
    "import configparser\n",
    "workingFolder=r'C:\\Users\\jfrancis\\OneDrive - GalaxE. Solutions, Inc\\GalaxE D Drive\\AI Journey\\Gen AI'\n",
    "# Read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(workingFolder+'\\\\config.ini')\n",
    "OPENAI_API_KEY=config.get('General','OPENAI_API_KEY')\n",
    "ACTIVELOOP_TOKEN=config.get('General','ACTIVELOOP_TOKEN')\n",
    "ACTIVELOOP_ORG_ID=config.get('General','ACTIVELOOP_ORG_ID')\n",
    "HUGGINGFACEHUB_API_TOKEN=config.get('General','HUGGINGFACEHUB_API_TOKEN')\n",
    "GOOGLE_API_KEY=config.get('General','GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID=config.get('General','GOOGLE_CSE_ID')\n",
    "COHERE_API_KEY=config.get('General','COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8b9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token from OPENAI/Active loop website before this. Now we are taking from the config.ini\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_ORG_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013131e7",
   "metadata": {},
   "source": [
    "### Similarity search and vector embeddings \n",
    "\n",
    "OpenAI offers a powerful language model called GPT-3, which can be used for various tasks, such as generating embeddings and performing similarity searches. In this example, we'll use the OpenAI API to generate embeddings for a set of documents and then perform a similarity search using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4acf2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query 'A cat is sitting on a mat.':\n",
      "The cat is on the mat.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600f719",
   "metadata": {},
   "source": [
    "We initialize the OpenAI API client by setting the OpenAI API key. This allows us to use OpenAI's services for generating embeddings.\n",
    "\n",
    "We then define a list of documents as strings. These documents are the text data we want to analyze for semantic similarity.\n",
    "\n",
    "In order to perform this analysis, we need to convert our documents into a format that our similarity computation algorithm can understand. This is where OpenAIEmbeddings class comes in. We use it to generate embeddings for each document, transforming them into vectors that represent their semantic content.\n",
    "\n",
    "Similarly, we also transform our query string into an embedding. The query string is the text we want to find the most similar document too.\n",
    "\n",
    "With our documents and query now in the form of embeddings, we compute the cosine similarity between the query embedding and each document embedding. The cosine similarity is a metric used to determine how similar two vectors are. In our case, it gives us a list of similarity scores for our query against each document.\n",
    "\n",
    "With our similarity scores in hand, we then identify the document most similar to our query. We do this by finding the index of the highest similarity score and retrieving the corresponding document from our list of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f7eb0",
   "metadata": {},
   "source": [
    "### Embedding Models\n",
    "\n",
    "Embedding models are a type of machine learning model that convert discrete data into continuous vectors. In the context of natural language processing, these discrete data points can be words, sentences, or even entire documents. The generated vectors, also known as embeddings, are designed to capture the semantic meaning of the original data.\n",
    "\n",
    "For instance, words that are semantically similar (e.g., 'cat' and 'kitten') would have similar embeddings. These embeddings are dense, which means that they use many dimensions (often hundreds) to capture nuances in meaning.\n",
    "\n",
    "The primary benefit of embeddings is that they allow us to use mathematical operations to reason about semantic meaning. For example, we can calculate the cosine similarity between two embeddings to assess how semantically similar the corresponding words or documents are.\n",
    "\n",
    "We initialize our embedding model. For this task, we've chosen the pre-trained \"sentence-transformers/all-mpnet-base-v2\" model. This model is designed to transform sentences into embeddings - vectors that encapsulate the semantic meaning of the sentences. The model_kwargs parameter is used here to specify that we want our computations to be performed on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4e005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence_transformers===2.2.2\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a7804",
   "metadata": {},
   "source": [
    "Now that we have our model, we define a list of documents - these are the pieces of text that we want to convert into semantic embeddings.\n",
    "\n",
    "With our model and documents ready, we move on to generate the embeddings. We do this by calling the embed_documents method on our HuggingFaceEmbeddings instance, passing our list of documents as an argument. This method processes each document and returns a corresponding list of embeddings.\n",
    "\n",
    "These embeddings are now ready for any downstream tasks such as classification, clustering, or similarity analysis. They represent our original documents in a form that machines can understand and process, enabling us to perform complex semantic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca70041",
   "metadata": {},
   "source": [
    "### Cohere embeddings\n",
    "\n",
    "Cohere is dedicated to making its innovative multilingual language models accessible to all, thereby democratizing advanced NLP technologies worldwide. Their Multilingual Model, which maps text into a semantic vector space for better text similarity understanding, significantly enhances multilingual applications such as search operations. Unlike their English language model, the multilingual model uses dot product computations resulting in superior performance. \n",
    "\n",
    "These multilingual embeddings are represented in a 768-dimensional vector space.\n",
    "\n",
    "To activate the power of the Cohere API, one needs to acquire an API key. Here's a step-by-step guide to doing so:\n",
    "\n",
    "    Visit the Cohere Dashboard.(https://dashboard.cohere.com/)\n",
    "    If you haven't already, you must either log in or sign up for a Cohere account. Please note that you agree to adhere to the Terms of Use and Privacy Policy by signing up.\n",
    "    When you're logged in, the dashboard provides an intuitive interface to create and manage your API keys.\n",
    "\n",
    "Once we have the API key, we initialize an instance of the CohereEmbeddings class within LangChain, specifying the \"embed-multilingual-v2.0\" model.\n",
    "\n",
    "We then specify a list of texts in various languages. The embed_documents() method is subsequently invoked to generate unique embeddings for each text in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19772e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello from Cohere!\n",
      "Embedding: [0.23461914, 0.50146484, -0.048828125, 0.13989258, -0.18029785]\n",
      "Text: مرحبًا من كوهير!\n",
      "Embedding: [0.25317383, 0.30004883, 0.0104904175, 0.12573242, -0.18273926]\n",
      "Text: Hallo von Cohere!\n",
      "Embedding: [0.10266113, 0.28320312, -0.050201416, 0.23706055, -0.07159424]\n",
      "Text: Bonjour de Cohere!\n",
      "Embedding: [0.15185547, 0.28173828, -0.057281494, 0.11743164, -0.04385376]\n",
      "Text: ¡Hola desde Cohere!\n",
      "Embedding: [0.25146484, 0.43139648, -0.0859375, 0.24682617, -0.11706543]\n",
      "Text: Olá do Cohere!\n",
      "Embedding: [0.18664551, 0.39038086, -0.045898438, 0.14562988, -0.11254883]\n",
      "Text: Ciao da Cohere!\n",
      "Embedding: [0.115722656, 0.43310547, -0.026168823, 0.14575195, 0.07080078]\n",
      "Text: 您好，来自 Cohere！\n",
      "Embedding: [0.24609375, 0.30859375, -0.111694336, 0.26635742, -0.051086426]\n",
      "Text: कोहेरे से नमस्ते!\n",
      "Embedding: [0.1932373, 0.6352539, 0.03213501, 0.117370605, -0.26098633]\n"
     ]
    }
   ],
   "source": [
    "#pip install cohere\n",
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "# COHERE_API_KEY=\"your_cohere_api_key\"\n",
    "cohere = CohereEmbeddings(\n",
    "    model=\"embed-multilingual-v2.0\",\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45843c",
   "metadata": {},
   "source": [
    "### Deep Lake Vector Store\n",
    "\n",
    "Vector stores are data structures or databases designed to store and manage high-dimensional vectors efficiently. They enable efficient similarity search, nearest neighbor search, and other vector-related operations. Vector stores can be built using various data structures such as approximate nearest neighbor (ANN) techniques, KD trees, or Vantage Point trees.\n",
    "\n",
    "Deep Lake, serves as both a data lake for deep learning and a multi-modal vector store. As a multi-modal vector store, it allows users to store images, audio, videos, text, and metadata in a format optimized for deep learning. It enables hybrid search, allowing users to search both embeddings and their attributes. \n",
    "\n",
    "Users can save data locally, in their cloud, or on Activeloop storage. Deep Lake supports the training of PyTorch and TensorFlow models while streaming data with minimal boilerplate code. It also provides features like version control, dataset queries, and distributed workloads using a simple Python API.\n",
    "\n",
    "Moreover, as the size of datasets increases, it becomes increasingly difficult to store them in local memory. A local vector store could have been utilized in this particular instance since only a few documents are being uploaded. However, the necessity for a centralized cloud dataset arises in a typical production setting, where thousands or millions of documents may be involved and accessed by various programs.\n",
    "\n",
    "Let’s see how to use Deep Lake for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d4d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360240b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fe0eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3b\\envs\\env_llm\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.4) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/jfrancis/langchain_course_embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://jfrancis/langchain_course_embeddings loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:25<00:00\n",
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://jfrancis/langchain_course_embeddings', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (4, 1536)  float32   None   \n",
      "    ids      text     (4, 1)      str     None   \n",
      " metadata    json     (4, 1)      str     None   \n",
      "   text      text     (4, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e1ef684c-81cd-11ee-be16-401c83da435e',\n",
       " 'e1ef684d-81cd-11ee-862b-401c83da435e',\n",
       " 'e1ef684e-81cd-11ee-8964-401c83da435e',\n",
       " 'e1ef684f-81cd-11ee-a3c2-401c83da435e']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "my_activeloop_dataset_name = \"langchain_course_embeddings\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40296c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d898166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Jordan was born on 17 February 1963.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# istantiate the llm wrapper\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# create the question-answering chain\n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# ask a question to the chain\n",
    "qa_chain.run(\"When was Michael Jordan born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8432234d",
   "metadata": {},
   "source": [
    "Let's break down each step to understand how these technologies work together.\n",
    "\n",
    "    OpenAI and LangChain Integration: LangChain, a library built for chaining NLP models, is designed to work seamlessly with OpenAI's GPT-3.5-turbo model for language understanding and generation. You've initialized OpenAI embeddings using OpenAIEmbeddings(), and these embeddings are later used to transform the text into a high-dimensional vector representation. This vector representation captures the semantic essence of the text and is essential for information retrieval tasks.\n",
    "    Deep Lake: Deep Lake is a Vector Store for creating, storing, and querying vector representations (also known as embeddings) of data.\n",
    "    Text Retrieval: Using the db.as_retriever() function, you've transformed the Deep Lake dataset into a retriever object. This object is designed to fetch the most relevant pieces of text from the dataset based on the semantic similarity of their embeddings.\n",
    "    Question Answering: The final step involves setting up a RetrievalQA chain from LangChain. This chain is designed to accept a natural language question, transform it into an embedding, retrieve the most relevant document chunks from the Deep Lake dataset, and generate a natural language answer. The ChatOpenAI model, which is the underlying model of this chain, is responsible for both the question embedding and the answer generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

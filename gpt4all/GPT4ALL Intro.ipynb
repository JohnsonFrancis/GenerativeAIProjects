{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edecc10",
   "metadata": {},
   "source": [
    "### How GPT4All works?\n",
    "\n",
    "It is trained on top of Facebook’s LLaMA model, which released its weights under a non-commercial license. Still, running the mentioned architecture on your local PC is impossible due to the large (7 billion) number of parameters. The authors incorporated two tricks to do efficient fine-tuning and inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c469c",
   "metadata": {},
   "source": [
    "### 1. Convert the Model\n",
    "\n",
    "The first step is to download the weights and use a script from the LLaMAcpp repository to convert the weights from the old format to the new one. It is a required step; otherwise, the LangChain library will not identify the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c3f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workingFolder=r'C:\\Users\\jfrancis\\OneDrive - GalaxE. Solutions, Inc\\GalaxE D Drive\\AI Journey\\Gen AI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ffa70",
   "metadata": {},
   "source": [
    "#### We need to download the weights file. You can either head to [https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/] and download the weights (make sure to download the one that ends with *.ggml.bin) or use the following Python snippet that breaks down the file into multiple chunks and downloads them gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "local_path = workingFolder + '\\\\gpt4all-lora-quantized-ggml.bin'\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
    "\n",
    "# send a GET request to the URL to download the file.\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response\n",
    "# to it in chunks.\n",
    "with open(local_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4f0ab",
   "metadata": {},
   "source": [
    "#### This process might take a while since the file size is 4GB. Then, it is time to transform the downloaded file to the latest format. We start by downloading the codes in the LLaMAcpp repository or simply fork it using the following command. (You need to have the git command installed) Pass the downloaded file to the convert.py script and run it with a Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219f431",
   "metadata": {},
   "source": [
    "\n",
    "<br>git clone https://github.com/ggerganov/llama.cpp.git\n",
    "<br>cd llama.cpp && git checkout 2b26469\n",
    "<br>python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56900ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece\n",
    "#pip install langchain==0.0.152  \n",
    "#pip install pyllamacpp==1.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87e1f7",
   "metadata": {},
   "source": [
    "### 2. Load the Model and Generate\n",
    "\n",
    "The LangChain library uses PyLLaMAcpp module to load the converted GPT4All weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c96eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97bced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040ca17",
   "metadata": {},
   "source": [
    "#### The template string defines the interaction’s overall structure. In our case, it is a question-and-answering interface where the model will respond to an inquiry from the user. There are two important parts:\n",
    "\n",
    "    Question: We declare the {question} placeholder and pass it as an input_variable to the template object to get initialized (by the user) later.\n",
    "    Answer: Based on our preference, it sets a behavior or style for the model’s generation process. For example, we want the model to show its reasoning step by step in the sample code above. There is an endless opportunity; it is possible to ask the model not to mention any detail, answer with one word, and be funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4ce087",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=workingFolder + '\\\\models\\\\ggml-model-q4_0.bin', callback_manager=callback_manager, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c421b258",
   "metadata": {},
   "source": [
    "#### The default behavior is to wait for the model to finish its inference process to print out its outputs. However, it could take more than an hour (depending on your hardware) to respond to one prompt because of the large number of parameters in the model. We can use the StreamingStdOutCallbackHandler() callback to instantly show the latest generated token. This way, we can be sure that the generation process is running and the model shows the expected behavior. Otherwise, it is possible to stop the inference and adjust the prompt.\n",
    "\n",
    "The GPT4All class is responsible for reading and initializing the weights file and setting the required callbacks. Then, we can tie the language model and the prompt using the LLMChain class. It will enable us to ask questions from the model using the run() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a891d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What happens when it rains somewhere?\n",
      "\n",
      "Answer: Let's think step by step. Whenever there is rain, the ground receives a lot of water and becomes wetter than usual; this can lead to flooding if enough rain occurs in one particular area or region for an extended period of time (such as during hurricanes). The surface tension properties of droplets that fall from clouds also determine whether they will bead up on leafy vegetation, clothes or any other objects nearby. This effect is seen when it rains lightly and the leaves in a tree start to look shiny with dew because the water molecules have been attracted due their electrical charges at surface level of droplets."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Question: What happens when it rains somewhere?\\n\\nAnswer: Let's think step by step. Whenever there is rain, the ground receives a lot of water and becomes wetter than usual; this can lead to flooding if enough rain occurs in one particular area or region for an extended period of time (such as during hurricanes). The surface tension properties of droplets that fall from clouds also determine whether they will bead up on leafy vegetation, clothes or any other objects nearby. This effect is seen when it rains lightly and the leaves in a tree start to look shiny with dew because the water molecules have been attracted due their electrical charges at surface level of droplets.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404b7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1564e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What happens when it rains somewhere?\n",
      "\n",
      "Answer: Let's think step by step. First, rain can fall from the sky as precipitation in various forms such as drops or larger spheres of water called \"raindrops\". Rainfall may cause flooding on land and could be dangerous for humans if they happen to walk underneath it with no shelter available. On sea level surfaces like oceans, rivers or lakes rainwater can also erode the soil's particles leading to sedimentation processes that create layers of muddy material called \"alluvial fans\" deposited at river mouths where tidal forces are usually weakest but still affecting them through wind and currents. Rainfall on mountainsides causes water runoff which feeds into rivers or drains directly onto the sea, contributing to coastline sedimentation (erosion) as well. Additionally it can cause landsliding leading to mudflows if there is too much rain for soil conditions that may create a dangerous flow of debris downstream from eroding hillsides and mountains into populated areas during stormy weather events causing damage, injuries or death due to floods in riverside habitats."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Question: What happens when it rains somewhere?\\n\\nAnswer: Let\\'s think step by step. First, rain can fall from the sky as precipitation in various forms such as drops or larger spheres of water called \"raindrops\". Rainfall may cause flooding on land and could be dangerous for humans if they happen to walk underneath it with no shelter available. On sea level surfaces like oceans, rivers or lakes rainwater can also erode the soil\\'s particles leading to sedimentation processes that create layers of muddy material called \"alluvial fans\" deposited at river mouths where tidal forces are usually weakest but still affecting them through wind and currents. Rainfall on mountainsides causes water runoff which feeds into rivers or drains directly onto the sea, contributing to coastline sedimentation (erosion) as well. Additionally it can cause landsliding leading to mudflows if there is too much rain for soil conditions that may create a dangerous flow of debris downstream from eroding hillsides and mountains into populated areas during stormy weather events causing damage, injuries or death due to floods in riverside habitats.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

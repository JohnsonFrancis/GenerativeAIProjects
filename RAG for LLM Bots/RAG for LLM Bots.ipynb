{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9c151a",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation for LLM Bots with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dad67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either you can store the  OpenAI key in the “OPENAI_API_KEY” environment variable.\n",
    "# or pass it here as below from a config.ini\n",
    "import configparser\n",
    "workingFolder=r'C:\\Users\\jfrancis\\OneDrive - GalaxE. Solutions, Inc\\GalaxE D Drive\\AI Journey\\Gen AI'\n",
    "# Read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(workingFolder+'\\\\config.ini')\n",
    "OPENAI_API_KEY=config.get('General','OPENAI_API_KEY')\n",
    "ACTIVELOOP_TOKEN=config.get('General','ACTIVELOOP_TOKEN')\n",
    "ACTIVELOOP_ORG_ID=config.get('General','ACTIVELOOP_ORG_ID')\n",
    "HUGGINGFACEHUB_API_TOKEN=config.get('General','HUGGINGFACEHUB_API_TOKEN')\n",
    "GOOGLE_API_KEY=config.get('General','GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID=config.get('General','GOOGLE_CSE_ID')\n",
    "COHERE_API_KEY=config.get('General','COHERE_API_KEY')\n",
    "APIFY_API_TOKEN=config.get('General','APIFY_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8aaafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token from OPENAI/Active loop website before this. Now we are taking from the config.ini\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "os.environ[\"APIFY_API_TOKEN\"] = APIFY_API_TOKEN\n",
    "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_ORG_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32df06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain==0.0.208\n",
    "#deeplake==3.6.5\n",
    "#openai==0.27.8\n",
    "#tiktoken==0.4.0\n",
    "#cohere==4.34.0\n",
    "#apify-client==1.5.0\n",
    "#streamlit==1.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4b3a3",
   "metadata": {},
   "source": [
    "### What is Retrieval Augmented Generation (RAG) in AI?\n",
    "\n",
    "Retrieval Augmented Generation, or RAG, is an advanced technique in AI that bridges information retrieval and text generation. It is designed to handle intricate and knowledge-intensive tasks by pulling relevant information from external sources and feeding it into a Large Language Model for text generation. When RAG receives an input, it searches for pertinent documents from specified sources (e.g., Wikipedia, company knowledge base, etc.), combines this retrieved data with the input, and then provides a comprehensive output with references. This innovative structure allows RAG to seamlessly integrate new and evolving information without retraining the entire model from scratch. It also enables you to fine-tune the model, enhancing its knowledge domain beyond what it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb451961",
   "metadata": {},
   "source": [
    "### Step 1: Loading the Data with RecursiveCharacterTextSplitter\n",
    "\n",
    "In this stage, we are gathering the data needed to provide context to the chatbot. We use ApifyLoader to scrape the content from a specific website. The RecursiveCharacterTextSplitter is then used to split the data into smaller, manageable chunks. Next, we embed the data using CohereEmbeddings which translates the text data into numerical data (vectors) that the chatbot can learn from. Lastly, we load the transformed data into Deep Lake.\n",
    "\n",
    "Helper Functions\n",
    "\n",
    "    ApifyWrapper(): Scrapes the content from websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40c35d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3b\\envs\\env_llm\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.4) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import ApifyDatasetLoader\n",
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.document_loaders.base import Document\n",
    "import os\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://python.langchain.com/docs/get_started/introduction\"}]},\n",
    "    dataset_mapping_function=lambda dataset_item: Document(\n",
    "        page_content=dataset_item[\"text\"] if dataset_item[\"text\"] else \"No content available\",\n",
    "        metadata={\n",
    "            \"source\": dataset_item[\"url\"],\n",
    "            \"title\": dataset_item[\"metadata\"][\"title\"]\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80be7300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tuesday, September 26, 2023 \\nThe sample return capsule on Sunday. \\nImage: NASA. \\nRelated articles\\n26 September 2023: NASA\\'s OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth\\n29 August 2023: US government sues SpaceX, claims hiring discrimination against asylees\\n2 July 2023: European Space Agency\\'s Euclid telescope launches from Florida, US\\n17 May 2023: Scientists: Rock that hit New Jersey home is 4.6 billion-year-old meteorite\\n9 May 2023: First NASA TROPICS satellites launch to monitor tropical storms\\nCollaborate!\\nPillars of Wikinews writing\\nWriting an article\\nYesterday, a capsule from NASA\\'s Origins, Spectral Interpretation, Resource Identification and Security – Regolith Explorer (OSIRIS-REx) spacecraft containing samples from the asteroid Bennu arrived in the US city of Houston, Texas. The capsule was en route to the Johnson Space Center (JSC) after landing in the Utah Test and Training Range the day before. This was NASA\\'s first asteroid sample return. \\nAs of Sunday, the capsule was in a cleanroom with the sample container undergoing a nitrogen purge, in which nitrogen is pumped into a container, maintaining the samples\\' purity from Earth\\'s atmosphere\\'s contamination of them. Staff are to later take apart the container, catalog its contents, and eventually provide samples to scientists in other institutions. \\nNASA Administrator Bill Nelson congratulated the OSIRIS-REx team \"on a picture-perfect mission — the first American asteroid sample return in history — which will deepen our understanding of the origin of our solar system and its formation.\" Nelson also said the samples will lead to better understanding of asteroids that could head for Earth. \\nOSIRIS-REx launched in September 2016 and arrived at Bennu in December 2018. For almost two years, NASA scientists worked to select the best sample collection site. The spacecraft collected samples in October 2020 as it used a robotic arm to make contact with the surface and then inject nitrogen gas to kick up material into its receptacle. About 8.8 ounces of the approximately six metric tons of debris entered the receptacle. OSIRIS-REx began its return trip to Earth in May 2021. \\nThe sample capsule was dropped at an altitude of 63,000 miles as the OSIRIS-REx spacecraft flew by Earth. It entered the atmosphere at 10:42 EDT (1442 UTC) and touched down about ten minutes later. Recovery teams used radar and other instruments to track the capsule. Personnel secured the capsule to ready its transport to JSC. \\nSince its launch in 2016, OSIRIS-REx has traveled 3.86 billion miles. With the sample-return phase complete, the spacecraft will continue on to the asteroid Apophis. To reflect the change in purpose, the mission has been renamed to Origins, Spectral Interpretation, Resource Identification, Security-APophis EXplorer (OSIRIS-APEX). \\nAn artist\\'s impression of OSIRIS-REx at Bennu. \\nImage: NASA / Goddard Space Flight Center. \\n101955 Bennu. \\nImage: NASA. \\nSources\\nMike Wall. \"OSIRIS-REx\\'s asteroid sample lands in Houston (photos)\" — Space.com, September 25, 2023\\nClaire O\\'Shea. \"NASA’s First Asteroid Sample Has Landed, Now Secure in Clean Room\" — NASA, September 24, 2023\\nAshley Strickland. \"A long-awaited asteroid sample has landed in the US\" — CNN, September 24, 2023\\nJeff Hecht. \"Asteroid Bennu Almost Swallowed Spacecraft Whole\" — Sky & Telescope, July 14, 2022', metadata={'source': 'https://en.wikinews.org/wiki/NASA%27s_OSIRIS-REx_arrives_in_Houston,_US_after_returning_asteroid_samples_to_Earth', 'title': \"NASA's OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth - Wikinews, the free news source\"})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc025fc",
   "metadata": {},
   "source": [
    "ApifyWrapperRecursiveCharacterTextSplitter(): Splits the scraped content into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da0f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# we split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=20, length_function=len\n",
    ")\n",
    "docs_split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6628d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Tuesday, September 26, 2023 \\nThe sample return capsule on Sunday. \\nImage: NASA. \\nRelated articles\\n26 September 2023: NASA's OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth\\n29 August 2023: US government sues SpaceX, claims hiring discrimination against asylees\\n2 July 2023: European Space Agency's Euclid telescope launches from Florida, US\\n17 May 2023: Scientists: Rock that hit New Jersey home is 4.6 billion-year-old meteorite\\n9 May 2023: First NASA TROPICS satellites launch to monitor tropical storms\\nCollaborate!\\nPillars of Wikinews writing\\nWriting an article\\nYesterday, a capsule from NASA's Origins, Spectral Interpretation, Resource Identification and Security – Regolith Explorer (OSIRIS-REx) spacecraft containing samples from the asteroid Bennu arrived in the US city of Houston, Texas. The capsule was en route to the Johnson Space Center (JSC) after landing in the Utah Test and Training Range the day before. This was NASA's first asteroid sample return.\", metadata={'source': 'https://en.wikinews.org/wiki/NASA%27s_OSIRIS-REx_arrives_in_Houston,_US_after_returning_asteroid_samples_to_Earth', 'title': \"NASA's OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth - Wikinews, the free news source\"}),\n",
       " Document(page_content='As of Sunday, the capsule was in a cleanroom with the sample container undergoing a nitrogen purge, in which nitrogen is pumped into a container, maintaining the samples\\' purity from Earth\\'s atmosphere\\'s contamination of them. Staff are to later take apart the container, catalog its contents, and eventually provide samples to scientists in other institutions. \\nNASA Administrator Bill Nelson congratulated the OSIRIS-REx team \"on a picture-perfect mission — the first American asteroid sample return in history — which will deepen our understanding of the origin of our solar system and its formation.\" Nelson also said the samples will lead to better understanding of asteroids that could head for Earth.', metadata={'source': 'https://en.wikinews.org/wiki/NASA%27s_OSIRIS-REx_arrives_in_Houston,_US_after_returning_asteroid_samples_to_Earth', 'title': \"NASA's OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth - Wikinews, the free news source\"}),\n",
       " Document(page_content='OSIRIS-REx launched in September 2016 and arrived at Bennu in December 2018. For almost two years, NASA scientists worked to select the best sample collection site. The spacecraft collected samples in October 2020 as it used a robotic arm to make contact with the surface and then inject nitrogen gas to kick up material into its receptacle. About 8.8 ounces of the approximately six metric tons of debris entered the receptacle. OSIRIS-REx began its return trip to Earth in May 2021. \\nThe sample capsule was dropped at an altitude of 63,000 miles as the OSIRIS-REx spacecraft flew by Earth. It entered the atmosphere at 10:42 EDT (1442 UTC) and touched down about ten minutes later. Recovery teams used radar and other instruments to track the capsule. Personnel secured the capsule to ready its transport to JSC.', metadata={'source': 'https://en.wikinews.org/wiki/NASA%27s_OSIRIS-REx_arrives_in_Houston,_US_after_returning_asteroid_samples_to_Earth', 'title': \"NASA's OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth - Wikinews, the free news source\"}),\n",
       " Document(page_content='Since its launch in 2016, OSIRIS-REx has traveled 3.86 billion miles. With the sample-return phase complete, the spacecraft will continue on to the asteroid Apophis. To reflect the change in purpose, the mission has been renamed to Origins, Spectral Interpretation, Resource Identification, Security-APophis EXplorer (OSIRIS-APEX). \\nAn artist\\'s impression of OSIRIS-REx at Bennu. \\nImage: NASA / Goddard Space Flight Center. \\n101955 Bennu. \\nImage: NASA. \\nSources\\nMike Wall. \"OSIRIS-REx\\'s asteroid sample lands in Houston (photos)\" — Space.com, September 25, 2023\\nClaire O\\'Shea. \"NASA’s First Asteroid Sample Has Landed, Now Secure in Clean Room\" — NASA, September 24, 2023\\nAshley Strickland. \"A long-awaited asteroid sample has landed in the US\" — CNN, September 24, 2023\\nJeff Hecht. \"Asteroid Bennu Almost Swallowed Spacecraft Whole\" — Sky & Telescope, July 14, 2022', metadata={'source': 'https://en.wikinews.org/wiki/NASA%27s_OSIRIS-REx_arrives_in_Houston,_US_after_returning_asteroid_samples_to_Earth', 'title': \"NASA's OSIRIS-REx arrives in Houston, US after returning asteroid samples to Earth - Wikinews, the free news source\"})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef234ac",
   "metadata": {},
   "source": [
    "\n",
    "    CohereEmbeddings(): Translates text data into numerical data.\n",
    "    DeepLake(): Stores and retrieves the transformed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2610f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://jfrancis/kb-material', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 4096)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['88eedeed-822f-11ee-b60b-401c83da435e',\n",
       " '88eedeee-822f-11ee-98a7-401c83da435e',\n",
       " '88eedeef-822f-11ee-b082-401c83da435e',\n",
       " '88eedef0-822f-11ee-b8f9-401c83da435e']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = CohereEmbeddings(model = \"embed-english-v2.0\")\n",
    "\n",
    "username = my_activeloop_org_id # replace with your username from app.activeloop.ai\n",
    "db_id = 'kb-material'# replace with your database name\n",
    "DeepLake.force_delete_by_path(f\"hub://{username}/{db_id}\")\n",
    "\n",
    "dbs = DeepLake(dataset_path=f\"hub://{username}/{db_id}\", embedding_function=embeddings)\n",
    "dbs.add_documents(docs_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87625002",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve Data\n",
    "\n",
    "In this step, we’re setting up the environment to retrieve data from DeepLake using the CohereEmbeddings for transforming numerical data back to text. We’ll then use ContextualCompressionRetriever & CohereRerank to search, rank and retrieve the relevant data.\n",
    "\n",
    "First we set the COHERE_API_KEY and ACTIVELOOP_TOKEN environment variables, allowing us to access the Cohere and ActiveLoop services.\n",
    "\n",
    "    DeepLake() retrieve data\n",
    "    CohereEmbeddings()\n",
    "\n",
    "Following this, we create a DeepLake object, passing in the dataset path to the DeepLake instance, setting it to read-only mode and passing in the embedding function.\n",
    "\n",
    "Next, we define a data_lake function. Inside this function, we instantiate a CohereEmbeddings object with a specific model, embed-english-v2.0.\n",
    "\n",
    "    ContextualCompressionRetriever() & CohereRerank()\n",
    "    Reranking (cohere.com)\n",
    "\n",
    "We then instantiate a CohereRerank object with a specific model and number of top items to consider (top_n), and finally create a ContextualCompressionRetriever object, passing in the compressor and retriever objects. The data_lake function returns the DeepLake object, the compression retriever, and the retriever.\n",
    "\n",
    "The data retrieval process is set up by calling the data_lake function and unpacking its return values into dbs, compression_retriever, and retriever.\n",
    "\n",
    "The Rerank endpoint acts as the last stage reranker of a search flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec52619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 19:49:54.087 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\ProgramData\\Anaconda3b\\envs\\env_llm\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://jfrancis/kb-material already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "@st.cache_resource()\n",
    "def data_lake():\n",
    "    embeddings = CohereEmbeddings(model = \"embed-english-v2.0\")\n",
    "\n",
    "    dbs = DeepLake(\n",
    "        dataset_path=\"hub://jfrancis/kb-material\", \n",
    "        read_only=True, \n",
    "        embedding_function=embeddings\n",
    "        )\n",
    "    retriever = dbs.as_retriever()\n",
    "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "    retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "    retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "    retriever.search_kwargs[\"k\"] = 20\n",
    "\n",
    "    compressor = CohereRerank(\n",
    "        model = 'rerank-english-v2.0',\n",
    "        top_n=5\n",
    "        )\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "        )\n",
    "    return dbs, compression_retriever, retriever\n",
    "\n",
    "dbs, compression_retriever, retriever = data_lake()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67641ece",
   "metadata": {},
   "source": [
    "### Step 3: Use ConversationBufferWindowMemory to Build Conversation Chain with Memory\n",
    "\n",
    "In this step, we will build a memory system for our chatbot using the ConversationBufferWindowMemory.\n",
    "\n",
    "The memory function instantiates a ConversationBufferWindowMemory object with a specific buffer size (k), a key for storing chat history, and parameters for returning messages and output key. The function returns the instantiated memory object.\n",
    "\n",
    "We then instantiate the memory by calling the memory function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c4b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "@st.cache_resource()\n",
    "def memory():\n",
    "    memory=ConversationBufferWindowMemory(\n",
    "        k=3,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True, \n",
    "        output_key='answer'\n",
    "        )\n",
    "    return memory\n",
    "\n",
    "memory=memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e1c84",
   "metadata": {},
   "source": [
    "The chatbot uses the ChatOpenAI() function to initiate our LLM Chat model. \n",
    "Next, we build the conversation chain using the ConversationalRetrievalChain. We use the from_llm class method, passing in the llm, retriever, memory, and several additional parameters. The resulting chain object is stored in the qa variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8099b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "llm=llm,\n",
    "retriever=compression_retriever,\n",
    "memory=memory,\n",
    "verbose=True,\n",
    "chain_type=\"stuff\",\n",
    "return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41bdf7",
   "metadata": {},
   "source": [
    "### Step 4: Building the Chat UI\n",
    "\n",
    "In this final step, we set up the chat user interface (UI).\n",
    "\n",
    "We start by creating a button that, when clicked, triggers the clearing of cache and session states, effectively starting a new chat session.\n",
    "\n",
    "Then, we initialize the chat history if it does not exist and display previous chat messages from the session state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part onwards you won't be able to run in Jupyter notebook.\n",
    "# You will need to save this entire code as .py file and run form CLI\n",
    "streamlit run name_of_your_chatbot.py #run with the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a button to trigger the clearing of cache and session states\n",
    "if st.sidebar.button(\"Start a New Chat Interaction\"):\n",
    "    st.clear_cache_and_session()\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6176532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "def capture_and_display_output(func: Callable[..., Any], args, **kwargs) -> Any:\n",
    "    # Capture the standard output\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = output_catcher = io.StringIO()\n",
    "\n",
    "    # Run the given function and capture its output\n",
    "    response = func(args, **kwargs)\n",
    "\n",
    "    # Reset the standard output to its original value\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "    # Clean the captured output\n",
    "    output_text = output_catcher.getvalue()\n",
    "    clean_text = re.sub(r\"\\x1b[.?[@-~]\", \"\", output_text)\n",
    "\n",
    "    # Custom CSS for the response box\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "        .response-value {\n",
    "            border: 2px solid #6c757d;\n",
    "            border-radius: 5px;\n",
    "            padding: 20px;\n",
    "            background-color: #f8f9fa;\n",
    "            color: #3d3d3d;\n",
    "            font-size: 20px;  # Change this value to adjust the text size\n",
    "            font-family: monospace;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    # Create an expander titled \"See Verbose\"\n",
    "    with st.expander(\"See Langchain Thought Process\"):\n",
    "        # Display the cleaned text in Streamlit as code\n",
    "        st.code(clean_text)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56db9dd",
   "metadata": {},
   "source": [
    "The chat_ui function is used to handle the chat interactions. Inside this function, we accept user input, add the user’s message to the chat history and display it, load the memory variables which include the chat history, and predict and display the chatbot’s response.\n",
    "\n",
    "The function also displays the top 2 retrieved sources relevant to the response and appends the chatbot’s response to the session state. The chat_ui function is then called, passing in the ConversationalRetrievalChain object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8919dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ui(qa):\n",
    "    # Accept user input\n",
    "    if prompt := st.chat_input(\n",
    "        \"Ask me questions: How can I retrieve data from Deep Lake in Langchain?\"\n",
    "    ):\n",
    "\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # Display user message in chat message container\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "\n",
    "        # Display assistant response in chat message container\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            full_response = \"\"\n",
    "\n",
    "            # Load the memory variables, which include the chat history\n",
    "            memory_variables = memory.load_memory_variables({})\n",
    "\n",
    "            # Predict the AI's response in the conversation\n",
    "            with st.spinner(\"Searching course material\"):\n",
    "                response = capture_and_display_output(\n",
    "                    qa, ({\"question\": prompt, \"chat_history\": memory_variables})\n",
    "                )\n",
    "\n",
    "            # Display chat response\n",
    "            full_response += response[\"answer\"]\n",
    "            message_placeholder.markdown(full_response + \"▌\")\n",
    "            message_placeholder.markdown(full_response)\n",
    "\n",
    "            #Display top 2 retrieved sources\n",
    "            source = response[\"source_documents\"][0].metadata\n",
    "            source2 = response[\"source_documents\"][1].metadata\n",
    "            with st.expander(\"See Resources\"):\n",
    "                st.write(f\"Title: {source['title'].split('·')[0].strip()}\")\n",
    "                st.write(f\"Source: {source['source']}\")\n",
    "                st.write(f\"Relevance to Query: {source['relevance_score'] * 100}%\")\n",
    "                st.write(f\"Title: {source2['title'].split('·')[0].strip()}\")\n",
    "                st.write(f\"Source: {source2['source']}\")\n",
    "                st.write(f\"Relevance to Query: {source2['relevance_score'] * 100}%\")\n",
    "\n",
    "        # Append message to session state\n",
    "        st.session_state.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": full_response}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f74e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function passing the ConversationalRetrievalChain\n",
    "chat_ui(qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484b3c8",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/JohnsonFrancis/GenerativeAIProjects/blob/main/RAG%20for%20LLM%20Bots/ChatApp1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad80cec",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/JohnsonFrancis/GenerativeAIProjects/blob/main/RAG%20for%20LLM%20Bots/ChatApp2.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
